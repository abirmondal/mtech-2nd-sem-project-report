Aiming to minimise human intervention in automatic story generation and maintain coherence between consecutive generated stories, one study \cite{kim_multi-modal_2023} proposed a novel multi-modal framework with automated storyline decision-making capabilities. This approach decomposes the process into three sequential stages for each paragraph: storyline prediction, text generation, and scene visualisation. The storyline, comprising characters, events, and places, is predicted by a BERT-based transformer encoder model leveraging a multiple-choice question-answering (MCQA) approach to select the most relevant entities from five candidates. Subsequently, a GPT-2-based transformer decoder model \cite{radford_language_2019} generates a paragraph based on the predicted storyline. Finally, a diffusion-based text-to-image model visualises a representative image for the scene. The framework was evaluated using both automated and human evaluations on a large-scale dataset, analysing the storyline guidance with metrics like Recall@1,2. The results demonstrated that this approach outperforms previous methods, showing the effectiveness of the storyline guidance in planning and leading to improved coherence in generated stories, both logically and visually. The provided sources did not explicitly detail specific limitations of this proposed framework.