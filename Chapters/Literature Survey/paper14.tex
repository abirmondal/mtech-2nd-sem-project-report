Research by Wang and Kreminski \cite{wang_guiding_2024} addresses the challenge that instruction-tuned Large Language Models (LLMs) can generate stories but often produce stories lacking diversity. They propose combining symbolic and neural approaches to guide and diversify story generation. Their method is presented as a variation on the "plan-and-write" approach, where a story outline is generated first, and then an LLM expands this outline into a complete story. A key aspect distinguishing their work is the method for generating the outline: instead of using language models (like those in Fan et al. \cite{fan_hierarchical_2018}) or fine-grained symbolic planning domains, they utilise a lightweight microtheory of storytelling goal sequencing encoded as an Answer Set Program (ASP). This ASP serves as a higher-level, more abstract symbolic specification to guide the high-level story structure, aiming to leverage the open-domain capabilities of LLMs while incorporating the structural and diversity benefits of a symbolic approach. The authors demonstrated that their ASP-based approach can produce more diverse stories than an unguided LLM, evidenced by semantic similarity analysis. They also illustrated the improved compactness and flexibility of ASP-based outline generation compared to full narrative planning through code excerpts. While the sources note the rigidity of traditional narrative planning as a challenge for open-domain generation, they do not explicitly detail specific limitations of their proposed ASP-based framework itself within the provided text.