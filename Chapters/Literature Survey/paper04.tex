Aiming to generate coherent and fluent passages of text about a given topic \cite{fan_hierarchical_2018}, one study proposed a hierarchical neural story generation approach. This method decomposed the task into generating a story premise using a convolutional language model, followed by transforming the premise into full text with a Seq2Seq model. Further enhancements included a novel model fusion mechanism to improve relevance to the prompt and a gated multi-scale self-attention mechanism for modelling long-range context. The models were trained on a large dataset comprising 300K human-written stories paired with writing prompts collected from an online forum. Results indicated large improvements over strong baselines in automated and human evaluations. Human judges favoured the generated stories two-to-one over a non-hierarchical approach for fluency, topicality, and overall quality. The specific limitations of this method were not explicitly detailed in the provided sources.