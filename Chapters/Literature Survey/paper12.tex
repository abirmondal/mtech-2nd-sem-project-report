Research by Khan et al. \cite{khan_storygenai_2023} explored creating computer systems capable of producing original narratives using conditional text generation based on keywords. This work specifically tackled the challenge of coherence in generated stories. The approach involved optimising a Hugging Face generative pre-trained model Version Two (GPT-2 \cite{radford_language_2019}), for this task, enabling conditional text generation up to five hundred words. The underlying model was pre-trained on a large dataset of 8 million website pages. The optimisation process included fine-tuning the top 6 layers while freezing the bottom 6, which also expedited training. The system uses the story title and an array of keywords as input, separated by unique tokens, with keywords sampled and shuffled during training to enhance generalisation. This process is conceptualised as expanding an input (title or starting phrase) into a piece of writing using keywords, aiming for the title's influence to extend throughout the text, contrasting with a raw language model that might just continue the initial sentence. Technical modifications included adjusting the mega-head self-attention mechanism to handle longer sequences while conserving memory. While the exact dataset used for fine-tuning or evaluation of this specific model wasn't explicitly detailed for this work in the provided excerpts, the pre-training data is mentioned. Experimental results reported a BLEU score of 0.704 averaged over ten genres, and indicated that generated sentences were semantically coherent, with initial sentences relating to the title. The sources also highlight general difficulties in artificial story generation, such as maintaining coherency, managing token/word limitations, generating compelling plot twists, and controlling story direction, along with issues like repetition. The focus on a maximum length of five hundred words in the method description might imply a limitation on the length of stories generated by this specific system.