Addressing the challenge of generating coherent and goal-directed narratives that overcome the tendency of language models to lose coherence in longer texts \cite{castricato_tell_2021}, one study proposed treating automated story generation as a generative question-answering problem. This novel approach works backwards from a final event, iteratively analysing the most recent event, generating a "why" question about a character's action, and creating a preceding event by answering this question. The system utilised the ELI5 QA model for generating candidate answers, which was trained on the Explain Like I'm Five Reddit corpus and used randomly selected documents from the Flash Fiction Online repository as reference. Candidate answers were ranked using a GPT-2 \cite{radford_language_2019} model fine-tuned on a science fiction summary corpus. Evaluating against a BART language model baseline fine-tuned to generate sentences in reversed order, the system demonstrated significant improvement, generating stories that were, on average, 15.9\% more coherent based on human evaluations measuring coherence entropy. The provided sources did not explicitly detail specific limitations of this proposed method.