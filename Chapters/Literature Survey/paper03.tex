Within the domain of statistical script learning, a significant contribution aimed at inferring held-out and novel events from text is presented in the work by Pichotta and Mooney \cite{pichotta_learning_2016}. Their approach involved parsing large natural language corpora, such as Wikipedia movie plot summaries (cleaned to remove extraneous syntax) and the NYT Gigaword corpus, to extract structured event sequences. Events were formally represented using a structured format, specifically a 5-tuple (v, es, eo, ep, p), capturing the verb, subject, direct object, prepositional object, and preposition. This representation was built upon the authors' prior work (Pichotta and Mooney 2014 \cite{pichotta_statistical_2014}) and was utilised to potentially reduce sparsity. To model these event sequences and predict event arguments, they employed a Recurrent Neural Network incorporating Long Short-Term Memory (LSTM) units. This architecture had previously demonstrated success on various AI tasks. The evaluation of their proposed model was conducted using the Narrative Cloze task, leveraging datasets derived from their parsed text collection and the ROCStories Cloze Task dataset. The experimental results showed that the LSTM-based model substantially outperformed prior approaches on both held-out and novel event inference tasks, demonstrating significant improvements compared to the previous state-of-the-art system, which was their 2D rewritten all-bigram model from 2014. The authors concluded that their LSTM-based model is effective at modelling and predicting event arguments and yields significant improvements in script inference capabilities. It is noted that their approach did not necessitate converting the learned event representations back into natural language text for the task, and the event representation process itself could potentially be lossy.