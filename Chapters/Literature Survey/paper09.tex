Addressing the need to make story generation systems goal-driven \cite{alabdulkarim_goal-directed_2021}, one research effort focused on enabling models to achieve a given final event or sentence. This was accomplished by training a Seq2Seq language model on event abstractions and fine-tuning it using reinforcement learning with a reward based on reaching a goal verb. This approach utilized reward shaping and was implemented as an update of a previous method by Tambwekar et al. (2019) \cite{tambwekar_controllable_2018} adapted to work on a GPT-2 \cite{radford_language_2019} 117M model fine-tuned on NER-replaced science fiction data. The resulting models were evaluated using event abstractions and the NER-replaced science fiction dataset. Results demonstrated that these models could achieve a given goal greater than 93\% of the time, and subjective ratings of story coherence improved significantly. A notable limitation of this method is that the event abstractions used are not human-readable without further processing, and a subsequent event-to-sentence step can sometimes negate the coherence gains.