
@inproceedings{sharma_ai_2025,
	address = {Cham},
	title = {{AI} {Based} {Story} {Generation}},
	isbn = {978-3-031-78128-5},
	doi = {10.1007/978-3-031-78128-5_3},
	abstract = {We utilized natural language processing (NLP) to craft a new story inspired by the iconic Harry Potter and Lord of the Rings series. Initially, we deployed LSTM for text generation that gave us limited set of results, then the capabilities of GPT-2 were explored, revealing challenges with grammatical correctness in the generated sentences. Subsequently, we integrated GPT-2, which significantly improved the quality of the story generation. GPT-2 not only produced logical narratives but also breathed life into the characters and plotlines, offering a rich source of inspiration for writers seeking novel ideas and fresh perspectives. We further used a stable diffusion model to create images that matched the generated story. Thus we attempted a well-rounded method to bring together both words and pictures, creating a storytelling sequence for writers and creators. The paper highlights how AI story generation can effectively help support the creation of stories in gaming, education and entertainment.},
	language = {en},
	booktitle = {Pattern {Recognition}},
	publisher = {Springer Nature Switzerland},
	author = {Sharma, Neha and Karwasra, Prashant and Sharma, Paritosh and Tahir, Md Arbaz},
	editor = {Antonacopoulos, Apostolos and Chaudhuri, Subhasis and Chellappa, Rama and Liu, Cheng-Lin and Bhattacharya, Saumik and Pal, Umapada},
	year = {2025},
	pages = {32--47},
}

@inproceedings{khan_storygenai_2023,
	title = {{StoryGenAI}: {An} {Automatic} {Genre}-{Keyword} {Based} {Story} {Generation}},
	shorttitle = {{StoryGenAI}},
	url = {https://ieeexplore.ieee.org/document/10183482},
	doi = {10.1109/CISES58720.2023.10183482},
	abstract = {Story Generation through Deep Learning is a fascinating area of research in Artificial Intelligence that aims to create computer systems that can produce original and compelling narratives and is an interesting concept that has flourished in the domain of Machine Learning applications starting from 2018. Most of the research carried out in this specific area has shown advances in Modelling and efficiency of story generation. However, some of the setbacks in Artificial Story Generation include little to no coherency with human generating pattern, tokens/words limitation, missing plot twists and direction of story. In this paper, we have performed a comparative study on Automatic Story Generation as well as the proposed scheme of this paper has main focus on generating a meaningful story with the help of conditional text generation using keywords upto five hundred words by optimizing hugging face generative pre trained model Version Two catering towards the problem of coherency in the text generated. As a result each sentence is semantically coherent and the first three sentences are indeed related to the title itself. The experimental results show a BLEU score of 0.704 averaging over ten genres.},
	urldate = {2025-01-17},
	booktitle = {2023 {International} {Conference} on {Computational} {Intelligence} and {Sustainable} {Engineering} {Solutions} ({CISES})},
	author = {Khan, Leah Pathan and Gupta, Vibhu and Bedi, Srishty and Singhal, Abhishek},
	month = apr,
	year = {2023},
	keywords = {Deep learning, Automatic Story Generation, Comparative Study, Computational intelligence, Computational modeling, Faces, GPT-2, Natural Language Generation, Natural Language Processing},
	pages = {955--960},
	file = {Khan et al_2023_StoryGenAI.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\CUUCMVNK\\Khan et al_2023_StoryGenAI.pdf:application/pdf},
}

@article{alhussain_automatic_2022,
	title = {Automatic {Story} {Generation}: {A} {Survey} of {Approaches}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Automatic {Story} {Generation}},
	url = {https://dl.acm.org/doi/10.1145/3453156},
	doi = {10.1145/3453156},
	abstract = {Computational generation of stories is a subfield of computational creativity where artificial intelligence and psychology intersect to teach computers how to mimic humans’ creativity. It helps generate many stories with minimum effort and customize the stories for the users’ education and entertainment needs. Although the automatic generation of stories started to receive attention many decades ago, advances in this field to date are less than expected and suffer from many limitations. This survey presents an extensive study of research in the area of non-interactive textual story generation, as well as covering resources, corpora, and evaluation methods that have been used in those studies. It also shed light on factors of story interestingness.},
	language = {en},
	number = {5},
	urldate = {2025-01-17},
	journal = {ACM Computing Surveys},
	author = {Alhussain, Arwa I. and Azmi, Aqil M.},
	month = jun,
	year = {2022},
	pages = {1--38},
	file = {Alhussain_Azmi_2022_Automatic Story Generation.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\U8UM2IDW\\Alhussain_Azmi_2022_Automatic Story Generation.pdf:application/pdf},
}

@article{kim_multi-modal_2023,
	title = {A {Multi}-{Modal} {Story} {Generation} {Framework} with {AI}-{Driven} {Storyline} {Guidance}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/6/1289},
	doi = {10.3390/electronics12061289},
	abstract = {An automatic story generation system continuously generates stories with a natural plot. The major challenge of automatic story generation is to maintain coherence between consecutive generated stories without the need for human intervention. To address this, we propose a novel multi-modal story generation framework that includes automated storyline decision-making capabilities. Our framework consists of three independent models: a transformer encoder-based storyline guidance model, which predicts a storyline using a multiple-choice question-answering problem; a transformer decoder-based story generation model that creates a story that describes the storyline determined by the guidance model; and a diffusion-based story visualization model that generates a representative image visually describing a scene to help readers better understand the story flow. Our proposed framework was extensively evaluated through both automatic and human evaluations, which demonstrate that our model outperforms the previous approach, suggesting the effectiveness of our storyline guidance model in making proper plans.},
	language = {en},
	number = {6},
	urldate = {2025-01-17},
	journal = {Electronics},
	author = {Kim, Juntae and Heo, Yoonseok and Yu, Hogeon and Nang, Jongho},
	month = jan,
	year = {2023},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {AI-driven storyline guidance, automatic story generation, diffusion, multi-modal story generation, multiple-choice question answering, story visualization},
	pages = {1289},
	file = {Kim et al_2023_A Multi-Modal Story Generation Framework with AI-Driven Storyline Guidance.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\PGZSS7Z8\\Kim et al_2023_A Multi-Modal Story Generation Framework with AI-Driven Storyline Guidance.pdf:application/pdf},
}

@misc{fan_hierarchical_2018,
	title = {Hierarchical {Neural} {Story} {Generation}},
	url = {http://arxiv.org/abs/1805.04833},
	doi = {10.48550/arXiv.1805.04833},
	abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
	month = may,
	year = {2018},
	note = {arXiv:1805.04833 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Fan et al_2018_Hierarchical Neural Story Generation.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\9H446T3B\\Fan et al_2018_Hierarchical Neural Story Generation.pdf:application/pdf},
}

@article{chhun_language_2024,
	title = {Do {Language} {Models} {Enjoy} {Their} {Own} {Stories}? {Prompting} {Large} {Language} {Models} for {Automatic} {Story} {Evaluation}},
	volume = {12},
	issn = {2307-387X},
	shorttitle = {Do {Language} {Models} {Enjoy} {Their} {Own} {Stories}?},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00689/124460/Do-Language-Models-Enjoy-Their-Own-Stories},
	doi = {10.1162/tacl_a_00689},
	abstract = {Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high-level human abilities such as creativity, reasoning, and deep understanding. Meanwhile, Large Language Models (LLMs) now achieve state-of-the-art performance on many NLP tasks. In this paper, we study whether LLMs can be used as substitutes for human annotators for ASE. We perform an extensive analysis of the correlations between LLM ratings, other automatic measures, and human annotations, and we explore the influence of prompting on the results and the explainability of LLM behaviour. Most notably, we find that LLMs outperform current automatic measures for system-level evaluation but still struggle at providing satisfactory explanations for their answers.},
	language = {en},
	urldate = {2025-01-17},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Chhun, Cyril and Suchanek, Fabian M. and Clavel, Chloé},
	month = sep,
	year = {2024},
	pages = {1122--1142},
	file = {Chhun et al. - 2024 - Do Language Models Enjoy Their Own Stories Prompt.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\TA47ZD5P\\Chhun et al. - 2024 - Do Language Models Enjoy Their Own Stories Prompt.pdf:application/pdf},
}

@article{ammanabrolu_story_2020,
	title = {Story {Realization}: {Expanding} {Plot} {Events} into {Sentences}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Story {Realization}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6232},
	doi = {10.1609/aaai.v34i05.6232},
	abstract = {Neural network based approaches to automated story plot generation attempt to learn how to generate novel plots from a corpus of natural language plot summaries. Prior work has shown that a semantic abstraction of sentences called events improves neural plot generation and and allows one to decompose the problem into: (1) the generation of a sequence of events (event-to-event) and (2) the transformation of these events into natural language sentences (event-to-sentence). However, typical neural language generation approaches to event-to-sentence can ignore the event details and produce grammatically-correct but semantically-unrelated sentences. We present an ensemble-based model that generates natural language guided by events. We provide results—including a human subjects study—for a full end-to-end automated story generation system showing that our method generates more coherent and plausible stories than baseline approaches 1.},
	language = {en},
	number = {05},
	urldate = {2025-01-17},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ammanabrolu, Prithviraj and Tien, Ethan and Cheung, Wesley and Luo, Zhaochen and Ma, William and Martin, Lara J. and Riedl, Mark O.},
	month = apr,
	year = {2020},
	note = {Number: 05},
	pages = {7375--7382},
	file = {Ammanabrolu et al_2020_Story Realization.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\EKBN6D8J\\Ammanabrolu et al_2020_Story Realization.pdf:application/pdf},
}

@article{fang_systematic_2023,
	title = {A systematic review of artificial intelligence technologies used for story writing},
	volume = {28},
	issn = {1573-7608},
	url = {https://doi.org/10.1007/s10639-023-11741-5},
	doi = {10.1007/s10639-023-11741-5},
	abstract = {With the digital revolution of artificial intelligence (AI) in language education, the way how people write and create stories has been transformed in recent years. Although recent studies have started to examine the roles of AI in literacy, there is a lack of systematic review to inform how it has been applied and what has been achieved in story-writing. This paper reviews the literature on the use of AI in story-writing during the last 5 years. The discussion explores the year of publication, countries of implementation, educational levels, participants and research methodology. In terms of research context, most studies were carried out in universities in the United States, and children and adult learners were the two most common participants. Most studies involved the collection and analysis of quantitative data. After that, the mechanisms of using AI for story-writing are investigated in terms of the types, approaches, and roles of AI. The pedagogies used in the learning context of AI-supported story-writing are discussed. Finally, the benefits of using AI in story-writing are pointed out. The findings show that the literature has paid most attention to learners’ creativity, writing skills, presentation skills, motivation, and satisfaction. The review also suggested that human-AI collaboration could effectively improve story creation. Some studies had trained high-level AI to help students write better stories. As findings from the current body of research are not conclusive, more work is needed in exploring challenges of using AI in story-writing. Lastly, a set of limitations and recommendations for future research are summarized in this study.},
	language = {en},
	number = {11},
	urldate = {2025-01-17},
	journal = {Education and Information Technologies},
	author = {Fang, Xiaoxuan and Ng, Davy Tsz Kit and Leung, Jac Ka Lok and Chu, Samuel Kai Wah},
	month = nov,
	year = {2023},
	keywords = {AI story generators, AI-supported story-writing, Digital Education and Educational Technology, Digital story writing, Human-AI story collaboration, Literature review},
	pages = {14361--14397},
	file = {Fang et al_2023_A systematic review of artificial intelligence technologies used for story.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\CBKMYUU4\\Fang et al_2023_A systematic review of artificial intelligence technologies used for story.pdf:application/pdf},
}

@article{jadhav_research_2024,
	title = {A {Research} {Study} on {How} {AI} {Creates} {Fiction} {Stories}},
	volume = {12},
	issn = {23219653},
	url = {https://www.ijraset.com/best-journal/a-research-study-on-how-ai-creates-fiction-stories},
	doi = {10.22214/ijraset.2024.59952},
	abstract = {Abstract: This paper includes the study of complex process by which artificial intelligence systems create fictional stories. In this process we cover the five processes such as The pre -processing steps for using AI to create a fictional story, Plot structure and plot production process for creating fictional stories using artificial intelligence, Character growth in the process of creating a virtual story using artificial intelligence, Generating dialogue in the process of creating a virtual story using artificial intelligence, and Using AI to revisit and iterate the process of creating fictional stories. Using state-of-the-art artificial intelligence models, especially those based on deep learning architectures such as recurrent neural networks (RNNs) and transformer models such as generative pre- trained transformers (GPTs), we analyse how these systems understand and manipulate language to create consistent fictional pieces. This paper This Make up story through empirical experiments and qualitative analysis, we carefully study the impact of different parameters, including Algorithm, Education, Fine-tuning Hint, Generation, and Evaluation methods, on the quality and diversity of the generated stories. The study also explores the role of artificial intelligence as well as human collaboration in the fiction stories writing and creative process. It discuss the Future Directions of AI’S role in creating fiction stories.},
	number = {4},
	urldate = {2025-01-17},
	journal = {International Journal for Research in Applied Science and Engineering Technology},
	author = {Jadhav, Harshada},
	month = apr,
	year = {2024},
	pages = {1066--1073},
	file = {Jadhav_2024_A Research Study on How AI Creates Fiction Stories.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\JJ2WRYTV\\Jadhav_2024_A Research Study on How AI Creates Fiction Stories.pdf:application/pdf},
}

@inproceedings{saddhono_new_2024,
	title = {A {New} way to {Create} {Virtual} {Authors}/{Writers} using {AI} {Based} {Technology} with {Optimized} {Voice} and {Style}},
	url = {https://ieeexplore.ieee.org/document/10616853/?arnumber=10616853},
	doi = {10.1109/ICACITE60783.2024.10616853},
	abstract = {The convergence of artificial intelligence (AI) with literature marks a new era in the creation and appreciation of literary works. It is an area exploring the frontier of AI technologies in crafting narratives deeply resonant with human experiences, while at the same time introducing novel literary voices and styles. “Development of AI from primitive text generators to such sophisticated systems that they are capable of actually producing works, in a way, assaulting our understanding of authorship and creativity. The paper provides an exhaustive view of the historical development that AI has had in literature, going through the journey from early experiments in algorithmic poetry to the current state-of-the-art models capable of composing full-length novels. Herein, I intend to consider the main stages of development of AI and the course of accompanying changes in methods for literary production, which will allow me to more profoundly take up the question of AI’s potential for the revolution in literary creativity. Central to this research is the question of how AI-generated literature opens up and expands the boundaries of new possibilities offered in terms of narrative structure, thematic exploration, and stylistic innovation. This paper draws key traits of AI-authored works distinct from works of authorial AI-produced through both qualitative and quantitative analysis of AI-produced texts. It extends the unconventionality of its plot development, integration with cultural and linguistic elements, and an ability to imitate yet simultaneously innovate prevailing literary styles that get acceptance.},
	urldate = {2025-01-17},
	booktitle = {2024 4th {International} {Conference} on {Advance} {Computing} and {Innovative} {Technologies} in {Engineering} ({ICACITE})},
	author = {Saddhono, Kundharu and Saputra, Nanda and {Herman} and Saragih, Bahagia and Sumbayak, Desri Maria and Sipayung, Rohdearni Wati},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Computational Text Analysis, Creativity, Cross-Cultural Comparison, Ethical Considerations, Generators, Human-AI Collaboration, Industries, Linguistics, Literature, Narrative Structure, Production, Reflection, Statistical analysis, Stylistic Innovation, Technological innovation, Virtual Authors},
	pages = {1509--1514},
	file = {Saddhono et al_2024_A New way to Create Virtual Authors-Writers using AI Based Technology with.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\JRIAIW8E\\Saddhono et al_2024_A New way to Create Virtual Authors-Writers using AI Based Technology with.pdf:application/pdf},
}

@article{lee_artificial_2023,
	title = {Artificial {Intelligence}-{Based} {Content} {Generator} {Technology} for {Young} {English}-as-a-{Foreign}-{Language} {Learners}’ {Reading} {Enjoyment}},
	volume = {54},
	issn = {0033-6882},
	url = {https://doi.org/10.1177/00336882231165060},
	doi = {10.1177/00336882231165060},
	abstract = {Artificial intelligence has recently seen tremendous growth, and been applied to several fields, including the second-language domain. In this article, we share an innovative practice based on one of potential artificial intelligence technologies for second-language teaching and learning – artificial intelligence-based content generator, which generates texts based on user’s keywords. In total, 121 young English-as-a-foreign-language learners participated in the study, with half of them having engaged in the artificial intelligence-based content generator-based activity, and the other half having received traditional English-as-a-foreign-language reading instruction. We examined the extent to which the artificial intelligence-based content generator-based activity could influence the participants’ foreign language enjoyment and interests in reading English books, and the participants were given the survey addressing these variables, prior to and after the innovative practice. It was found that the condition based on the artificial intelligence-based content generator-based activity was more effective in terms of enhancing the target variables, and that the group which engaged in the artificial intelligence-based content generator-based activity was largely in favor of artificial intelligence-based content generator technology. Pedagogical implications for employing this technology in second-language contexts are provided.},
	language = {en},
	number = {2},
	urldate = {2025-01-17},
	journal = {RELC Journal},
	author = {Lee, Jang Ho and Shin, Dongkwang and Noh, Wonjun},
	month = aug,
	year = {2023},
	note = {Publisher: SAGE Publications Ltd},
	pages = {508--516},
	file = {PDF:C\:\\Users\\Abir Mondal\\Zotero\\storage\\NQN3L5Y8\\Lee et al. - 2023 - Artificial Intelligence-Based Content Generator Technology for Young English-as-a-Foreign-Language L.pdf:application/pdf},
}

@article{castricato_tell_2021,
	title = {Tell {Me} {A} {Story} {Like} {I}'m {Five}: {Story} {Generation} via {Question} {Answering}},
	shorttitle = {Tell {Me} {A} {Story} {Like} {I}'m {Five}},
	url = {https://par.nsf.gov/biblio/10249509-tell-me-story-like-five-story-generation-via-question-answering},
	abstract = {Neural language model-based approaches to automated story generation suffer from two important limitations. First, language model-based story generators generally do not work toward a given goal or ending. Second, they often lose coherence as the story gets longer. We propose a novel approach to automated story generation that treats the problem as one of generative question-answering. Our proposed story generation system starts with sentences encapsulating the ﬁnal event of the story. The system then iteratively (1) analyzes the text describing the most recent event, (2) generates a question about “why” a character is doing the thing they are doing in the event, and then (3) attempts to generate another, preceding event by answering this question. We show that the coherency of a story can be measured as the relative entropy over the distribution of responses to claims about said story’s events. Using a within-subjects human evaluation we measure this coherency entropy over the responses to sets of True-False statements for multiple stories generated by our model and each baseline. The evaluation shows that our system generates stories that are on average 15.9\% more coherent that those generated by the BART [Lewis et al., 2019] language model ﬁne-tuned on a story corpus to generate sentences in reversed order to more closely match our process.},
	language = {en},
	urldate = {2025-01-17},
	journal = {Proceedings of the 3rd Workshop on Narrative Understanding},
	author = {Castricato, Louis and Frazier, Spencer and Balloch, Jonathan and Riedl, Mark and Tarakad, Nitya},
	month = jan,
	year = {2021},
	file = {Castricato et al_2021_Tell Me A Story Like I'm Five.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\WSVXI8KN\\Castricato et al_2021_Tell Me A Story Like I'm Five.pdf:application/pdf},
}

@article{pellas_effects_2023,
	title = {The {Effects} of {Generative} {AI} {Platforms} on {Undergraduates}’ {Narrative} {Intelligence} and {Writing} {Self}-{Efficacy}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7102},
	url = {https://www.mdpi.com/2227-7102/13/11/1155},
	doi = {10.3390/educsci13111155},
	abstract = {Digital storytelling and generative artificial intelligence (AI) platforms have emerged as transformative tools that empower individuals to write with confidence and share their stories effectively. However, a research gap exists in understanding the effects of using such web-based platforms on narrative intelligence and writing self-efficacy. This study aims to investigate whether digital story creation tasks on web-based platforms can influence the narrative intelligence and writing self-efficacy of undergraduate students. A pretest–posttest comparison study between two groups was conducted with sixty-four undergraduate students (n = 64), majoring in Primary Education. More specifically, it compares the effects of the most well-known conventional platforms, such as Storybird, Storyjumper, and ZooBurst (control condition), and generative AI platforms, such as Sudowrite, Jasper, and Shortly AI (experimental condition) on undergraduate students, with an equal distribution in each group. The findings indicate that the utilization of generative AI platforms in the context of story creation tasks can substantially enhance both narrative intelligence scores and writing self-efficacy when compared to conventional platforms. Nonetheless, there was no significant difference in the creative identity factor. Generative AI platforms have promising implications for supporting undergraduates’ narrative intelligence and writing self-efficacy in fostering their story creation design and development.},
	language = {en},
	number = {11},
	urldate = {2025-01-17},
	journal = {Education Sciences},
	author = {Pellas, Nikolaos},
	month = nov,
	year = {2023},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, digital platforms, narrative intelligence, storytelling, writing self-efficacy},
	pages = {1155},
	file = {Pellas_2023_The Effects of Generative AI Platforms on Undergraduates’ Narrative.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\ESCA4KUZ\\Pellas_2023_The Effects of Generative AI Platforms on Undergraduates’ Narrative.pdf:application/pdf},
}

@misc{wang_guiding_2024,
	title = {Guiding and {Diversifying} {LLM}-{Based} {Story} {Generation} via {Answer} {Set} {Programming}},
	url = {http://arxiv.org/abs/2406.00554},
	doi = {10.48550/arXiv.2406.00554},
	abstract = {Instruction-tuned large language models (LLMs) are capable of generating stories in response to open-ended user requests, but the resulting stories tend to be limited in their diversity. Older, symbolic approaches to story generation (such as planning) can generate substantially more diverse plot outlines, but are limited to producing stories that recombine a fixed set of hand-engineered character action templates. Can we combine the strengths of these approaches while mitigating their weaknesses? We propose to do so by using a higher-level and more abstract symbolic specification of high-level story structure -- implemented via answer set programming (ASP) -- to guide and diversify LLM-based story generation. Via semantic similarity analysis, we demonstrate that our approach produces more diverse stories than an unguided LLM, and via code excerpts, we demonstrate the improved compactness and flexibility of ASP-based outline generation over full-fledged narrative planning.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Wang, Phoebe J. and Kreminski, Max},
	month = jun,
	year = {2024},
	note = {arXiv:2406.00554 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Wang_Kreminski_2024_Guiding and Diversifying LLM-Based Story Generation via Answer Set Programming.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\RZFAW39Q\\Wang_Kreminski_2024_Guiding and Diversifying LLM-Based Story Generation via Answer Set Programming.pdf:application/pdf},
}

@inproceedings{suh_luminate_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {Luminate: {Structured} {Generation} and {Exploration} of {Design} {Space} with {Large} {Language} {Models} for {Human}-{AI} {Co}-{Creation}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {Luminate},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642400},
	doi = {10.1145/3613904.3642400},
	abstract = {Thanks to their generative capabilities, large language models (LLMs) have become an invaluable tool for creative processes. These models have the capacity to produce hundreds and thousands of visual and textual outputs, offering abundant inspiration for creative endeavors. But are we harnessing their full potential? We argue that current interaction paradigms fall short, guiding users towards rapid convergence on a limited set of ideas, rather than empowering them to explore the vast latent design space in generative models. To address this limitation, we propose a framework that facilitates the structured generation of design space in which users can seamlessly explore, evaluate, and synthesize a multitude of responses. We demonstrate the feasibility and usefulness of this framework through the design and development of an interactive system, Luminate, and a user study with 14 professional writers. Our work advances how we interact with LLMs for creative tasks, introducing a way to harness the creative potential of LLMs.},
	urldate = {2025-01-17},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Suh, Sangho and Chen, Meng and Min, Bryan and Li, Toby Jia-Jun and Xia, Haijun},
	month = may,
	year = {2024},
	pages = {1--26},
	file = {Suh et al_2024_Luminate.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\UJXC4CYR\\Suh et al_2024_Luminate.pdf:application/pdf},
}

@inproceedings{bradley_quality-diversity_2023,
	title = {Quality-{Diversity} through {AI} {Feedback}},
	url = {https://openreview.net/forum?id=owokKCrGYr},
	abstract = {In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through {\textbackslash}emph\{AI feedback\}, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-QD controls. Further, human evaluation of QDAIF-generated creative texts validates reasonable agreement between AI and human evaluation. Our results thus highlight the potential of AI feedback to guide open-ended search for creative and original solutions, providing a recipe that seemingly generalizes to many domains and modalities. In this way, QDAIF is a step towards AI systems that can independently search, diversify, evaluate, and improve, which are among the core skills underlying human society's capacity for innovation.},
	language = {en},
	urldate = {2025-01-17},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Bradley, Herbie and Dai, Andrew and Teufel, Hannah Benita and Zhang, Jenny and Oostermeijer, Koen and Bellagente, Marco and Clune, Jeff and Stanley, Kenneth and Schott, Gregory and Lehman, Joel},
	month = oct,
	year = {2023},
	file = {Bradley et al_2023_Quality-Diversity through AI Feedback.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\G696GYLW\\Bradley et al_2023_Quality-Diversity through AI Feedback.pdf:application/pdf},
}

@article{rivera_story_2024,
	title = {The {Story} {So} {Far} on {Narrative} {Planning}},
	volume = {34},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2334-0843},
	url = {https://ojs.aaai.org/index.php/ICAPS/article/view/31509},
	doi = {10.1609/icaps.v34i1.31509},
	abstract = {Narrative planning is the use of automated planning to construct, communicate, and understand stories, a form of information to which human cognition and enaction is pre-disposed. We review the narrative planning problem in a manner suitable as an introduction to the area, survey different plan-based methodologies and affordances for reasoning about narrative, and discuss open challenges relevant to the broader AI community.},
	language = {en},
	urldate = {2025-01-17},
	journal = {Proceedings of the International Conference on Automated Planning and Scheduling},
	author = {Rivera, Rogelio E. Cardona and Jhala, Arnav and Porteous, Julie and Young, R. Michael},
	month = may,
	year = {2024},
	pages = {489--499},
	file = {Rivera et al_2024_The Story So Far on Narrative Planning.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\9NPU34DI\\Rivera et al_2024_The Story So Far on Narrative Planning.pdf:application/pdf},
}

@inproceedings{chakrabarty_art_2024,
	address = {Honolulu HI USA},
	title = {Art or {Artifice}? {Large} {Language} {Models} and the {False} {Promise} of {Creativity}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {Art or {Artifice}?},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642731},
	doi = {10.1145/3613904.3642731},
	language = {en},
	urldate = {2025-01-17},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Chakrabarty, Tuhin and Laban, Philippe and Agarwal, Divyansh and Muresan, Smaranda and Wu, Chien-Sheng},
	month = may,
	year = {2024},
	pages = {1--34},
	file = {Chakrabarty et al_2024_Art or Artifice.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\33MKX4KG\\Chakrabarty et al_2024_Art or Artifice.pdf:application/pdf},
}

@article{martin_event_2018,
	title = {Event {Representations} for {Automated} {Story} {Generation} with {Deep} {Neural} {Nets}},
	volume = {32},
	issn = {2374-3468, 2159-5399},
	url = {http://arxiv.org/abs/1706.01331},
	doi = {10.1609/aaai.v32i1.11430},
	abstract = {Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.},
	number = {1},
	urldate = {2025-01-24},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Martin, Lara J. and Ammanabrolu, Prithviraj and Wang, Xinyu and Hancock, William and Singh, Shruti and Harrison, Brent and Riedl, Mark O.},
	month = apr,
	year = {2018},
	note = {arXiv:1706.01331 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Martin et al_2018_Event Representations for Automated Story Generation with Deep Neural Nets.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\2XGHQ5HH\\Martin et al_2018_Event Representations for Automated Story Generation with Deep Neural Nets.pdf:application/pdf;Snapshot:C\:\\Users\\Abir Mondal\\Zotero\\storage\\DQH8A4QP\\1706.html:text/html},
}

@misc{alabdulkarim_goal-directed_2021,
	title = {Goal-{Directed} {Story} {Generation}: {Augmenting} {Generative} {Language} {Models} with {Reinforcement} {Learning}},
	shorttitle = {Goal-{Directed} {Story} {Generation}},
	url = {http://arxiv.org/abs/2112.08593},
	doi = {10.48550/arXiv.2112.08593},
	abstract = {The advent of large pre-trained generative language models has provided a common framework for AI story generation via sampling the model to create sequences that continue the story. However, sampling alone is insufficient for story generation. In particular, it is hard to direct a language model to create stories to reach a specific goal event. We present two automated techniques grounded in deep reinforcement learning and reward shaping to control the plot of computer-generated stories. The first utilizes proximal policy optimization to fine-tune an existing transformer-based language model to generate text continuations but also be goal-seeking. The second extracts a knowledge graph from the unfolding story, which is used by a policy network with graph attention to select a candidate continuation generated by a language model. We report on automated metrics pertaining to how often stories achieve a given goal event as well as human participant rankings of coherence and overall story quality compared to baselines and ablations.},
	urldate = {2025-01-24},
	publisher = {arXiv},
	author = {Alabdulkarim, Amal and Li, Winston and Martin, Lara J. and Riedl, Mark O.},
	month = dec,
	year = {2021},
	note = {arXiv:2112.08593 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Alabdulkarim et al_2021_Goal-Directed Story Generation.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\3TK9C3M5\\Alabdulkarim et al_2021_Goal-Directed Story Generation.pdf:application/pdf;Snapshot:C\:\\Users\\Abir Mondal\\Zotero\\storage\\ESP5RPMX\\2112.html:text/html},
}

@inproceedings{akoury_storium_2020,
	address = {Online},
	title = {{STORIUM}: {A} {Dataset} and {Evaluation} {Platform} for {Machine}-in-the-{Loop} {Story} {Generation}},
	shorttitle = {{STORIUM}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.525},
	doi = {10.18653/v1/2020.emnlp-main.525},
	language = {en},
	urldate = {2025-02-10},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Akoury, Nader and Wang, Shufan and Whiting, Josh and Hood, Stephen and Peng, Nanyun and Iyyer, Mohit},
	year = {2020},
	pages = {6470--6484},
	file = {Akoury et al_2020_STORIUM.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\AADCZK7H\\Akoury et al_2020_STORIUM.pdf:application/pdf},
}

@article{chen_incorporating_2019,
	title = {Incorporating {Structured} {Commonsense} {Knowledge} in {Story} {Completion}},
	volume = {33},
	copyright = {https://www.aaai.org},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5183},
	doi = {10.1609/aaai.v33i01.33016244},
	abstract = {The ability to select an appropriate story ending is the first step towards perfect narrative comprehension. Story ending prediction requires not only the explicit clues within the context, but also the implicit knowledge (such as commonsense) to construct a reasonable and consistent story. However, most previous approaches do not explicitly use background commonsense knowledge. We present a neural story ending selection model that integrates three types of information: narrative sequence, sentiment evolution and commonsense knowledge. Experiments show that our model outperforms state-ofthe-art approaches on a public dataset, ROCStory Cloze Task (Mostafazadeh et al. 2017), and the performance gain from adding the additional commonsense knowledge is significant.},
	number = {01},
	urldate = {2025-02-10},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Jiaao and Chen, Jianshu and Yu, Zhou},
	month = jul,
	year = {2019},
	pages = {6244--6251},
	file = {Chen et al_2019_Incorporating Structured Commonsense Knowledge in Story Completion.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\FF4CRDYN\\Chen et al_2019_Incorporating Structured Commonsense Knowledge in Story Completion.pdf:application/pdf},
}

@article{tambwekar_controllable_2018,
	title = {Controllable {Neural} {Story} {Plot} {Generation} via {Reward} {Shaping}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1809.10736},
	doi = {10.48550/ARXIV.1809.10736},
	abstract = {Language-modeling--based approaches to story plot generation attempt to construct a plot by sampling from a language model (LM) to predict the next character, word, or sentence to add to the story. LM techniques lack the ability to receive guidance from the user to achieve a specific goal, resulting in stories that don't have a clear sense of progression and lack coherence. We present a reward-shaping technique that analyzes a story corpus and produces intermediate rewards that are backpropagated into a pre-trained LM in order to guide the model towards a given goal. Automated evaluations show our technique can create a model that generates story plots which consistently achieve a specified goal. Human-subject studies show that the generated stories have more plausible event ordering than baseline plot generation techniques.},
	urldate = {2025-02-10},
	journal = {CoRR},
	author = {Tambwekar, Pradyumna and Dhuliawala, Murtaza and Martin, Lara J. and Mehta, Animesh and Harrison, Brent and Riedl, Mark O.},
	year = {2018},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
	file = {Tambwekar et al_2018_Controllable Neural Story Plot Generation via Reward Shaping.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\QH42S2ZD\\Tambwekar et al_2018_Controllable Neural Story Plot Generation via Reward Shaping.pdf:application/pdf},
}

@inproceedings{tang_textbox_2022,
	address = {Abu Dhabi, UAE},
	title = {{TextBox} 2.0: {A} {Text} {Generation} {Library} with {Pre}-trained {Language} {Models}},
	shorttitle = {{TextBox} 2.0},
	url = {https://aclanthology.org/2022.emnlp-demos.42},
	doi = {10.18653/v1/2022.emnlp-demos.42},
	language = {en},
	urldate = {2025-03-11},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Tang, Tianyi and Li, Junyi and Chen, Zhipeng and Hu, Yiwen and Yu, Zhuohao and Dai, Wenxun and Zhao, Wayne Xin and Nie, Jian-yun and Wen, Ji-rong},
	year = {2022},
	pages = {435--444},
	file = {Tang et al_2022_TextBox 2.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\5ETZG9JJ\\Tang et al_2022_TextBox 2.pdf:application/pdf},
}

@inproceedings{li_textbox_2021,
	address = {Online},
	title = {{TextBox}: {A} {Unified}, {Modularized}, and {Extensible} {Framework} for {Text} {Generation}},
	shorttitle = {{TextBox}},
	url = {https://aclanthology.org/2021.acl-demo.4},
	doi = {10.18653/v1/2021.acl-demo.4},
	language = {en},
	urldate = {2025-03-11},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Junyi and Tang, Tianyi and He, Gaole and Jiang, Jinhao and Hu, Xiaoxuan and Xie, Puzhao and Chen, Zhipeng and Yu, Zhuohao and Zhao, Wayne Xin and Wen, Ji-Rong},
	year = {2021},
	pages = {30--39},
	file = {Li et al_2021_TextBox.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\E7BZFMLA\\Li et al_2021_TextBox.pdf:application/pdf},
}

@misc{tang_ngep_2022,
	title = {{NGEP}: {A} {Graph}-based {Event} {Planning} {Framework} for {Story} {Generation}},
	shorttitle = {{NGEP}},
	url = {http://arxiv.org/abs/2210.10602},
	doi = {10.48550/arXiv.2210.10602},
	abstract = {To improve the performance of long text generation, recent studies have leveraged automatically planned event structures (i.e. storylines) to guide story generation. Such prior works mostly employ end-to-end neural generation models to predict event sequences for a story. However, such generation models struggle to guarantee the narrative coherence of separate events due to the hallucination problem, and additionally the generated event sequences are often hard to control due to the end-to-end nature of the models. To address these challenges, we propose NGEP, an novel event planning framework which generates an event sequence by performing inference on an automatically constructed event graph and enhances generalisation ability through a neural event advisor. We conduct a range of experiments on multiple criteria, and the results demonstrate that our graph-based neural framework outperforms the state-of-the-art (SOTA) event planning approaches, considering both the performance of event sequence generation and the effectiveness on the downstream task of story generation.},
	urldate = {2025-03-24},
	publisher = {arXiv},
	author = {Tang, Chen and Zhang, Zhihao and Loakman, Tyler and Lin, Chenghua and Guerin, Frank},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10602 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Snapshot:C\:\\Users\\Abir Mondal\\Zotero\\storage\\F27J4X2Q\\2210.html:text/html;Tang et al_2022_NGEP.pdf:C\:\\Users\\Abir Mondal\\Zotero\\storage\\I87SZ7KF\\Tang et al_2022_NGEP.pdf:application/pdf},
}

@article{pichotta_learning_2016,
	title = {Learning {Statistical} {Scripts} with {LSTM} {Recurrent} {Neural} {Networks}},
	volume = {30},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10347},
	doi = {10.1609/aaai.v30i1.10347},
	abstract = {Scripts encode knowledge of prototypical sequences of events. We describe a Recurrent Neural Network model for statistical script learning using Long Short-Term Memory, an architecture which has been demonstrated to work well on a range of Artificial Intelligence tasks. We evaluate our system on two tasks, inferring held-out events from text and inferring novel events from text, substantially outperforming prior approaches on both tasks.},
	number = {1},
	urldate = {2025-04-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Pichotta, Karl and Mooney, Raymond},
	month = mar,
	year = {2016},
	file = {Full Text:C\:\\Users\\Abir Mondal\\Zotero\\storage\\RG6KCJMU\\Pichotta and Mooney - 2016 - Learning Statistical Scripts with LSTM Recurrent Neural Networks.pdf:application/pdf},
}

@misc{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	doi = {10.48550/arXiv.1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = dec,
	year = {2014},
	note = {arXiv:1409.3215 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Abir Mondal\\Zotero\\storage\\BBPHJBAZ\\Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf;Snapshot:C\:\\Users\\Abir Mondal\\Zotero\\storage\\5PU8HMDA\\1409.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2025-04-02},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {PDF:C\:\\Users\\Abir Mondal\\Zotero\\storage\\48KRLQCM\\Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf:application/pdf},
}

@inproceedings{pichotta_statistical_2014,
	address = {Gothenburg, Sweden},
	title = {Statistical {Script} {Learning} with {Multi}-{Argument} {Events}},
	url = {http://aclweb.org/anthology/E14-1024},
	doi = {10.3115/v1/E14-1024},
	language = {en},
	urldate = {2025-04-02},
	booktitle = {Proceedings of the 14th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Pichotta, Karl and Mooney, Raymond},
	year = {2014},
	pages = {220--229},
	file = {Full Text:C\:\\Users\\Abir Mondal\\Zotero\\storage\\PLJ2DRI3\\Pichotta and Mooney - 2014 - Statistical Script Learning with Multi-Argument Events.pdf:application/pdf},
}

@inproceedings{pichotta_using_2016,
	address = {Berlin, Germany},
	title = {Using {Sentence}-{Level} {LSTM} {Language} {Models} for {Script} {Inference}},
	url = {http://aclweb.org/anthology/P16-1027},
	doi = {10.18653/v1/P16-1027},
	language = {en},
	urldate = {2025-04-03},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Pichotta, Karl and Mooney, Raymond J.},
	year = {2016},
	pages = {279--289},
	file = {Full Text:C\:\\Users\\Abir Mondal\\Zotero\\storage\\H6AAXQKZ\\Pichotta and Mooney - 2016 - Using Sentence-Level LSTM Language Models for Script Inference.pdf:application/pdf},
}

@article{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	volume = {1},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	number = {8},
	journal = {OpenAI blog},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	month = feb,
	year = {2019},
	pages = {9},
	file = {PDF:C\:\\Users\\Abir Mondal\\Zotero\\storage\\LVH4TRWX\\Radford et al. - Language Models are Unsupervised Multitask Learners.pdf:application/pdf},
}
